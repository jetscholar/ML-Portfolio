{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3447a78",
   "metadata": {},
   "source": [
    "# A Simple Neural Network\n",
    "\n",
    "- [PolyCode](https://www.youtube.com/watch?v=kft1AJ9WVDk&t=210s)\n",
    "\n",
    "<img src=\"DataSet.png\" />\n",
    "\n",
    "**If input 1 is 1, then output is 1**\n",
    "\n",
    "<img src=\"perceptron.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42009663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47d7c28",
   "metadata": {},
   "source": [
    "## The Sigmoid Normalizing Function\n",
    "\n",
    "<img src=\"sigmoid.png\" />\n",
    "\n",
    "<p>In deep learning, a normalizing function, often referred to as normalization, serves the purpose of transforming the input data or intermediate representations in a way that helps the neural network learn more effectively and efficiently. Normalization techniques aim to address issues related to training convergence, generalization, and optimization by ensuring that the data or **activations** have certain desirable statistical properties.\n",
    "\n",
    "Normalization is particularly important when dealing with deep neural networks that have multiple layers, as the activations at different layers can exhibit large variations in magnitude, which can lead to challenges during training.</p>\n",
    "\n",
    "The purposes of these normalization techniques include:\n",
    "\n",
    "**Stabilizing Learning:** Normalization helps prevent gradients from becoming too large or too small, which can lead to training instability (exploding or vanishing gradients) and slow convergence.\n",
    "\n",
    "**Faster Convergence:** Normalization can lead to faster convergence by providing more consistent gradients and avoiding saturation of activation functions.\n",
    "\n",
    "**Regularization:** Normalization acts as a form of regularization by reducing the likelihood of overfitting. It can help prevent individual neurons from dominating the learning process.\n",
    "\n",
    "**Generalization:** Normalization can improve the generalization of a model by reducing the sensitivity of the network to small changes in input data.\n",
    "\n",
    "**Allowing Higher Learning Rates:** With normalized activations, it's often possible to use higher learning rates during training, which can speed up convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f665fae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the Sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddcfb6a",
   "metadata": {},
   "source": [
    "<img src=\"training.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01f4a07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
